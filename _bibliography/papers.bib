---
---

@string{aps = {American Physical Society,}}

@INPROCEEDINGS{RLALIGN,
  abbr={BIBE},
  author={Kinattinkara Ramakrishnan, Ramchalam and Singh, Jaspal and Blanchette, Mathieu},
  booktitle={2018 IEEE 18th International Conference on Bioinformatics and Bioengineering (BIBE)}, 
  title={RLALIGN: A Reinforcement Learning Approach for Multiple Sequence Alignment}, 
  year={2018},
  volume={},
  number={},
  pages={61-66},
  keywords={Games;Bioinformatics;Genomics;Training;Heuristic algorithms;Computer science;bioinformatics;machine learning;reinforcement learning;multiple sequence alignment},
  PDF={10.1109/BIBE.2018.00019}
  }

@Article{quantization_study,
      abbr={TinyML Symposium},
      title={An Empirical Study of Low Precision Quantization for TinyML}, 
      author={Shaojie Zhuo and Hongyu Chen and Kinattinkara Ramakrishnan, Ramchalam and Tommy Chen and Chen Feng and Yicheng Lin and Parker Zhang and Liang Shen},
      year={2022},
      eprint={2203.05492},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      PDF={https://arxiv.org/abs/2203.05492}, 
}

@Article{dmp_pruning,
      abbr={arXiv},
      title={Differentiable Mask for Pruning Convolutional and Recurrent Networks}, 
      author={Kinattinkara Ramakrishnan, Ramchalam and Eyy√ºb Sari and Vahid Partovi Nia},
      year={2020},
      eprint={1909.04567},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      PDF={https://arxiv.org/abs/1909.04567}, 
}



@inproceedings{ff,
abbr={NeurIPS},
 author = {Feng, Chen and Zhuo, Shaojie and  Kinattinkara Ramakrishnan, Ramchalam and Zhang, Xiaopeng and Yuan, Zhaocong and Li, Andrew Zou},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {94851--94870},
 publisher = {Curran Associates, Inc.},
 title = {Stepping Forward on the Last Mile},
 PDF = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ac6c452efdf063644d330caba2ab1e57-Paper-Conference.pdf},
 volume = {37},
 year = {2024},
 venue = {NeurIPS},
 selected = {true}
}

@inproceedings{omnidraft,
      abbr={NeurIPS},
      title={OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding}, 
      author={Kinattinkara Ramakrishnan, Ramchalam and Zhaocong Yuan and Shaojie Zhuo and Chen Feng and Yicheng Lin and Chenzheng Su and Xiaopeng Zhang},
      booktitle = {Advances in Neural Information Processing Systems},
      year={2025},
      eprint={2507.02659},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      PDF={https://arxiv.org/abs/2507.02659}, 
      ABSTRACT={Speculative decoding generally dictates having a small, efficient draft model that is either 
      pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 
      1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, 
      a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid 
      distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. 
      OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights 
      the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning 
      on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models 
      for speculative decoding; and additionally provides up to 1.5-2x speedup.},
      venue = {NeurIPS},
      selected = {true}
}

@misc{edgeASR,
      abbr={arXiv},
      title={Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition Models}, 
      author={Chen Feng and Yicheng Lin and Shaojie Zhuo and Chenzheng Su and Kinattinkara Ramakrishnan, Ramchalam and Zhaocong Yuan and Xiaopeng Zhang},
      year={2025},
      eprint={2507.07877},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      PDF={https://arxiv.org/abs/2507.07877}, 
}

@thesis{thesis,
      abbr={Masters Thesis},
      title={Exploring Reinforcement Learning Techniques in Multiple Sequence Alignment}, 
      author={Kinattinkara Ramakrishnan, Ramchalam},
      year={2018},
      PDF={https://escholarship.mcgill.ca/downloads/0c483m421}, 
}

@article{DBLP:journals/corr/abs-2006-05442,
  abbr={arXiv},
  author       = {Alejandro Murua and
                  Kinattinkara Ramakrishnan, Ramchalam and
                  Xinlin Li and
                  Rui Heng Yang and
                  Vahid Partovi Nia},
  title        = {Tensor train decompositions on recurrent networks},
  journal      = {CoRR},
  volume       = {abs/2006.05442},
  year         = {2020},
  PDF          = {https://arxiv.org/abs/2006.05442},
  eprinttype    = {arXiv},
  eprint       = {2006.05442},
  timestamp    = {Sat, 13 Jun 2020 18:28:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-05442.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{demosaicing,
abbr={ICIAR},
author="Kinattinkara Ramakrishnan, Ramchalam
and Jui Shangling
and Vahid Partovi Nia",
editor="Karray, Fakhri
and Campilho, Aur{\'e}lio
and Yu, Alfred",
title="Deep Demosaicing for Edge Implementation",
booktitle="Image Analysis and Recognition",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="275--286",
abstract="Most digital cameras use sensors coated with a Color Filter Array (CFA) to capture channel components at every pixel location, resulting in a mosaic image that does not contain pixel values in all channels. Current research on reconstructing these missing channels, also known as demosaicing, introduces many artifacts, such as zipper effect and false color. Many deep learning demosaicing techniques outperform other classical techniques in reducing the impact of artifacts. However, most of these models tend to be over-parametrized. Consequently, edge implementation of the state-of-the-art deep learning-based demosaicing algorithms on low-end edge devices is a major challenge. We provide an exhaustive search of deep neural network architectures and obtain a Pareto front of Color Peak Signal to Noise Ratio (CPSNR) as the performance criterion versus the number of parameters as the model complexity that outperforms the state-of-the-art. Architectures on the Pareto front can then be used to choose the best architecture for a variety of resource constraints. Simple architecture search methods such as exhaustive search and grid search requires some conditions of the loss function to converge to the optimum. We clarify these conditions in a brief theoretical study.",
isbn="978-3-030-27202-9",
pdf=https://link.springer.com/chapter/10.1007/978-3-030-27202-9_25
}

@INPROCEEDINGS{MSA,
  abbr={BIBE},
  author={Singh, Jaspal and Kinattinkara Ramakrishnan, Ramchalam and Blanchette, Mathieu},
  booktitle={2018 IEEE 18th International Conference on Bioinformatics and Bioengineering (BIBE)}, 
  title={[Regular Paper] Detection of Errors in Multi-genome Alignments Using Machine Learning Approaches}, 
  year={2018},
  volume={},
  number={},
  pages={47-53},
  keywords={Bioinformatics;Genomics;Vegetation;Tools;Machine learning;Phylogeny;Task analysis;bioinformatics, machine learning, evolution, multiple sequence alignment, artificial neural networks},
  pdf={10.1109/BIBE.2018.00017}}
